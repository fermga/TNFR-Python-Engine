{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32fd366f",
   "metadata": {},
   "source": [
    "# Interaction Sequences: TNFR-friendly telemetry and export\n",
    "\n",
    "Este cuaderno genera secuencias de interacción reproducibles, telemetría sintética y artefactos (figuras CSV/JSON) y escribe una guía en docs/INTERACTIONS_GUIDE.md. Sigue los principios TNFR: prioriza coherencia, trazabilidad y reproducibilidad. Las figuras y tablas se exportan a docs/assets/interactions/.\n",
    "\n",
    "Requisitos clave:\n",
    "- Semilla global fija para reproducibilidad\n",
    "- Exportar manifest con rutas y metadatos\n",
    "- Validar invariantes simples de las series (no-negatividad, timestamps monótonos)\n",
    "- Generar una guía Markdown con las imágenes y enlaces a tablas\n",
    "\n",
    "## 1) Configurar entorno y rutas de proyecto\n",
    "\n",
    "Detectamos la raíz del repositorio (git o heurística con pathlib), creamos carpetas de salida y parametrizamos rutas para activos (PNG/SVG/CSV/JSON).\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Detect repo root via git, else fallback to current working directory\n",
    "try:\n",
    "    import subprocess\n",
    "    root_guess = Path.cwd()\n",
    "    git_root = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], cwd=root_guess).decode().strip()\n",
    "    REPO_ROOT = Path(git_root)\n",
    "except Exception:\n",
    "    # Heurística: buscar pyproject.toml hacia arriba\n",
    "    p = Path.cwd()\n",
    "    while p != p.parent:\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            REPO_ROOT = p\n",
    "            break\n",
    "        p = p.parent\n",
    "    else:\n",
    "        REPO_ROOT = Path.cwd()\n",
    "\n",
    "# Ensure src is importable during interactive runs\n",
    "SRC_DIR = REPO_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "DOCS_DIR = REPO_ROOT / \"docs\"\n",
    "ASSETS_DIR = DOCS_DIR / \"assets\" / \"interactions\"\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "REPORTS_DIR = REPO_ROOT / \"results\" / \"reports\"\n",
    "\n",
    "for d in [DOCS_DIR, ASSETS_DIR, DATA_DIR, REPORTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Assets dir: {ASSETS_DIR}\")\n",
    "\n",
    "## 2) Importar librerías y establecer semilla\n",
    "\n",
    "Importamos librerías mínimas y fijamos semilla global para reproducibilidad.\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import math\n",
    "import hashlib\n",
    "import platform\n",
    "import random\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    # Optional: richer version reporting\n",
    "    import pkg_resources  # type: ignore\n",
    "except Exception:\n",
    "    pkg_resources = None\n",
    "\n",
    "# Fixed seeds for reproducibility\n",
    "SEED = int(os.environ.get(\"INTERACTIONS_SEED\", 1337))\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print({\"numpy_seed\": SEED})\n",
    "\n",
    "## 3) Cargar o simular telemetría base\n",
    "\n",
    "Implementamos carga desde CSV/JSON en data/ y un simulador de series sintéticas (tiempo, latencia, throughput, errores) con ruido controlado por la semilla.\n",
    "\n",
    "def load_telemetry(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    if path.suffix.lower() in {\".json\", \".jsonl\"}:\n",
    "        if path.suffix.lower() == \".jsonl\":\n",
    "            rows = [json.loads(line) for line in path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "            return pd.DataFrame(rows)\n",
    "        return pd.DataFrame(json.loads(path.read_text(encoding=\"utf-8\")))\n",
    "    raise ValueError(f\"Unsupported telemetry format: {path.suffix}\")\n",
    "\n",
    "\n",
    "def simulate_telemetry(n: int = 300, base_latency_ms: float = 120.0, base_rps: float = 50.0,\n",
    "                        error_p: float = 0.01, spike_every: int | None = 60,\n",
    "                        duration_s: int = 300, step_label: str = \"base\",\n",
    "                        start_ts: datetime | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Generate synthetic telemetry with gentle trends and occasional spikes.\n",
    "    Columns: ts, t_s, step, latency_ms, throughput_rps, error_rate.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(SEED + hash(step_label) % 10000)\n",
    "    if start_ts is None:\n",
    "        start_ts = datetime.utcnow()\n",
    "    times = np.linspace(0, duration_s, n)\n",
    "    ts = [start_ts + timedelta(seconds=float(t)) for t in times]\n",
    "\n",
    "    # Latency with low-frequency drift and noise\n",
    "    drift = 0.2 * np.sin(2 * np.pi * times / max(1.0, duration_s / 5))\n",
    "    noise = rng.normal(0, 5.0, size=n)\n",
    "    latency = np.clip(base_latency_ms * (1.0 + 0.01 * drift) + noise, 1.0, None)\n",
    "\n",
    "    # Throughput with mild trend and anti-correlation to latency\n",
    "    throughput = np.clip(base_rps * (1.0 - 0.002 * drift) + rng.normal(0, 2.0, size=n), 0.0, None)\n",
    "\n",
    "    # Error rate baseline with rare spikes\n",
    "    errors = np.clip(rng.binomial(1, error_p, size=n) * rng.uniform(0.2, 0.8, size=n), 0.0, 1.0)\n",
    "\n",
    "    if spike_every:\n",
    "        for i in range(n):\n",
    "            if i % spike_every == 0 and i > 0:\n",
    "                latency[i] *= rng.uniform(1.2, 1.6)\n",
    "                throughput[i] *= rng.uniform(0.6, 0.9)\n",
    "                errors[i] = max(errors[i], rng.uniform(0.05, 0.2))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"ts\": ts,\n",
    "        \"t_s\": times,\n",
    "        \"step\": step_label,\n",
    "        \"latency_ms\": latency,\n",
    "        \"throughput_rps\": throughput,\n",
    "        \"error_rate\": errors,\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "# quick smoke\n",
    "_sim = simulate_telemetry(n=50, step_label=\"smoke\")\n",
    "_sim.head()\n",
    "\n",
    "## 4) Definir secuencias de interacción canónicas\n",
    "\n",
    "Creamos dataclasses y un mini-DSL YAML/Dict para describir pasos (acción, duración, parámetros).\n",
    "Incluimos tres casos canónicos: 'login-browse-purchase', 'search-filter-paginate', 'retry-on-error'.\n",
    "\n",
    "@dataclass\n",
    "class InteractionStep:\n",
    "    action: str\n",
    "    duration_s: int\n",
    "    params: Dict[str, Any]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InteractionSequence:\n",
    "    name: str\n",
    "    steps: list[InteractionStep]\n",
    "\n",
    "\n",
    "def parse_sequences_from_yaml(yaml_text: str) -> list[InteractionSequence]:\n",
    "    doc = yaml.safe_load(yaml_text)\n",
    "    sequences: list[InteractionSequence] = []\n",
    "    for entry in doc.get(\"sequences\", []):\n",
    "        steps = [InteractionStep(s[\"action\"], int(s.get(\"duration_s\", 60)), s.get(\"params\", {})) for s in entry[\"steps\"]]\n",
    "        sequences.append(InteractionSequence(name=entry[\"name\"], steps=steps))\n",
    "    return sequences\n",
    "\n",
    "\n",
    "CANONICAL_YAML = \"\"\"\n",
    "sequences:\n",
    "  - name: login-browse-purchase\n",
    "    steps:\n",
    "      - action: login\n",
    "        duration_s: 60\n",
    "        params: { base_latency_ms: 180, base_rps: 30, error_p: 0.02 }\n",
    "      - action: browse\n",
    "        duration_s: 120\n",
    "        params: { base_latency_ms: 120, base_rps: 60, error_p: 0.01 }\n",
    "      - action: purchase\n",
    "        duration_s: 90\n",
    "        params: { base_latency_ms: 150, base_rps: 40, error_p: 0.03 }\n",
    "  - name: search-filter-paginate\n",
    "    steps:\n",
    "      - action: search\n",
    "        duration_s: 100\n",
    "        params: { base_latency_ms: 140, base_rps: 55, error_p: 0.015 }\n",
    "      - action: filter\n",
    "        duration_s: 80\n",
    "        params: { base_latency_ms: 160, base_rps: 45, error_p: 0.02 }\n",
    "      - action: paginate\n",
    "        duration_s: 140\n",
    "        params: { base_latency_ms: 130, base_rps: 65, error_p: 0.012 }\n",
    "  - name: retry-on-error\n",
    "    steps:\n",
    "      - action: call\n",
    "        duration_s: 60\n",
    "        params: { base_latency_ms: 110, base_rps: 70, error_p: 0.005 }\n",
    "      - action: error\n",
    "        duration_s: 30\n",
    "        params: { base_latency_ms: 220, base_rps: 20, error_p: 0.15, spike_every: 10 }\n",
    "      - action: retry\n",
    "        duration_s: 60\n",
    "        params: { base_latency_ms: 130, base_rps: 55, error_p: 0.02 }\n",
    "\"\"\"\n",
    "\n",
    "SEQUENCES = parse_sequences_from_yaml(CANONICAL_YAML)\n",
    "[name for name in [s.name for s in SEQUENCES]]\n",
    "\n",
    "## 5) Ejecutar secuencias y registrar telemetría\n",
    "\n",
    "Ejecutamos cada paso contra el simulador y agregamos DataFrames con etiquetas y timestamps continuos.\n",
    "\n",
    "def run_sequence(seq: InteractionSequence, start_ts: datetime | None = None) -> pd.DataFrame:\n",
    "    frames: list[pd.DataFrame] = []\n",
    "    cur_ts = start_ts or datetime.utcnow()\n",
    "    t_offset = 0.0\n",
    "    for step in seq.steps:\n",
    "        params = dict(step.params)\n",
    "        params.setdefault(\"duration_s\", step.duration_s)\n",
    "        df = simulate_telemetry(\n",
    "            n=max(50, int(params[\"duration_s\"]) // 1),\n",
    "            base_latency_ms=float(params.get(\"base_latency_ms\", 120.0)),\n",
    "            base_rps=float(params.get(\"base_rps\", 50.0)),\n",
    "            error_p=float(params.get(\"error_p\", 0.01)),\n",
    "            spike_every=params.get(\"spike_every\"),\n",
    "            duration_s=int(params[\"duration_s\"]),\n",
    "            step_label=step.action,\n",
    "            start_ts=cur_ts,\n",
    "        )\n",
    "        # Rebase t_s to cumulative timeline\n",
    "        df[\"t_s\"] = df[\"t_s\"] + t_offset\n",
    "        frames.append(df)\n",
    "        # Advance current ts and offset\n",
    "        cur_ts = df[\"ts\"].iloc[-1]\n",
    "        t_offset = float(df[\"t_s\"].iloc[-1])\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    out[\"sequence\"] = seq.name\n",
    "    return out\n",
    "\n",
    "\n",
    "# Example run\n",
    "_df = run_sequence(SEQUENCES[0])\n",
    "_df.head()\n",
    "\n",
    "## 6) Graficar figuras/curvas de telemetría\n",
    "\n",
    "Generamos curvas de latencia, throughput y tasa de errores; devolvemos objetos matplotlib Figure para exportación.\n",
    "\n",
    "def plot_telemetry(df: pd.DataFrame, title: str = \"\") -> dict[str, plt.Figure]:\n",
    "    figs: dict[str, plt.Figure] = {}\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Latency over time\n",
    "    fig1, ax1 = plt.subplots(figsize=(9, 4))\n",
    "    for step, g in df.groupby(\"step\"):\n",
    "        ax1.plot(g[\"t_s\"], g[\"latency_ms\"], label=step, alpha=0.9)\n",
    "    ax1.set_title(title or f\"Latency over time — {df['sequence'].iloc[0]}\")\n",
    "    ax1.set_xlabel(\"t [s]\")\n",
    "    ax1.set_ylabel(\"latency [ms]\")\n",
    "    ax1.legend()\n",
    "    figs[\"latency\"] = fig1\n",
    "\n",
    "    # Throughput over time\n",
    "    fig2, ax2 = plt.subplots(figsize=(9, 4))\n",
    "    for step, g in df.groupby(\"step\"):\n",
    "        ax2.plot(g[\"t_s\"], g[\"throughput_rps\"], label=step, alpha=0.9)\n",
    "    ax2.set_title(title or f\"Throughput over time — {df['sequence'].iloc[0]}\")\n",
    "    ax2.set_xlabel(\"t [s]\")\n",
    "    ax2.set_ylabel(\"throughput [rps]\")\n",
    "    ax2.legend()\n",
    "    figs[\"throughput\"] = fig2\n",
    "\n",
    "    # Error rate over time\n",
    "    fig3, ax3 = plt.subplots(figsize=(9, 4))\n",
    "    for step, g in df.groupby(\"step\"):\n",
    "        ax3.plot(g[\"t_s\"], g[\"error_rate\"], label=step, alpha=0.9)\n",
    "    ax3.set_title(title or f\"Error rate over time — {df['sequence'].iloc[0]}\")\n",
    "    ax3.set_xlabel(\"t [s]\")\n",
    "    ax3.set_ylabel(\"error rate [0..1]\")\n",
    "    ax3.legend()\n",
    "    figs[\"errors\"] = fig3\n",
    "\n",
    "    # Distributions\n",
    "    fig4, ax4 = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    sns.kdeplot(df[\"latency_ms\"], fill=True, ax=ax4[0])\n",
    "    ax4[0].set_title(\"Latency dist\")\n",
    "    sns.kdeplot(df[\"throughput_rps\"], fill=True, ax=ax4[1])\n",
    "    ax4[1].set_title(\"Throughput dist\")\n",
    "    sns.kdeplot(df[\"error_rate\"], fill=True, ax=ax4[2])\n",
    "    ax4[2].set_title(\"Error rate dist\")\n",
    "    fig4.suptitle(title or f\"Distributions — {df['sequence'].iloc[0]}\")\n",
    "    figs[\"distributions\"] = fig4\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return figs\n",
    "\n",
    "\n",
    "_figs = plot_telemetry(_df)\n",
    "plt.close('all')  # Avoid duplicate inline rendering when exporting\n",
    "\n",
    "## 7) Validar resultados y reproducibilidad\n",
    "\n",
    "Calculamos hashes de parámetros/datos, registramos versión de dependencias y semilla, y afirmamos invariantes básicos.\n",
    "\n",
    "def df_hash(df: pd.DataFrame) -> str:\n",
    "    m = hashlib.sha256()\n",
    "    # Stable hash: round floats; serialize selected columns\n",
    "    cols = [\"t_s\", \"latency_ms\", \"throughput_rps\", \"error_rate\", \"step\", \"sequence\"]\n",
    "    sub = df[cols].copy()\n",
    "    for c in [\"t_s\", \"latency_ms\", \"throughput_rps\", \"error_rate\"]:\n",
    "        sub[c] = pd.to_numeric(sub[c], errors=\"coerce\").round(6)\n",
    "    payload = sub.to_csv(index=False).encode(\"utf-8\")\n",
    "    m.update(payload)\n",
    "    return m.hexdigest()\n",
    "\n",
    "\n",
    "def validate_df(df: pd.DataFrame) -> dict[str, Any]:\n",
    "    assert (df[\"t_s\"].diff().fillna(0) >= 0).all(), \"Timestamps must be non-decreasing\"\n",
    "    for c in [\"latency_ms\", \"throughput_rps\", \"error_rate\"]:\n",
    "        assert (df[c] >= 0).all(), f\"{c} must be non-negative\"\n",
    "    return {\n",
    "        \"rows\": int(len(df)),\n",
    "        \"duration_s\": float(df[\"t_s\"].iloc[-1] - df[\"t_s\"].iloc[0]) if len(df) > 1 else 0.0,\n",
    "        \"hash\": df_hash(df),\n",
    "    }\n",
    "\n",
    "\n",
    "ENV_INFO = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "if pkg_resources is not None:\n",
    "    try:\n",
    "        ENV_INFO[\"numpy\"] = pkg_resources.get_distribution(\"numpy\").version\n",
    "        ENV_INFO[\"pandas\"] = pkg_resources.get_distribution(\"pandas\").version\n",
    "        ENV_INFO[\"matplotlib\"] = pkg_resources.get_distribution(\"matplotlib\").version\n",
    "        ENV_INFO[\"seaborn\"] = pkg_resources.get_distribution(\"seaborn\").version\n",
    "        ENV_INFO[\"pyyaml\"] = pkg_resources.get_distribution(\"pyyaml\").version\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "ENV_INFO\n",
    "\n",
    "## 8) Exportar figuras y artefactos a docs/assets/\n",
    "\n",
    "Guardamos figuras (PNG/SVG), tablas (CSV) y un manifiesto con rutas y metadatos para cada secuencia.\n",
    "\n",
    "def export_sequence(df: pd.DataFrame, figs: dict[str, plt.Figure], out_dir: Path) -> dict[str, Any]:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    manifest: dict[str, Any] = {\n",
    "        \"sequence\": df[\"sequence\"].iloc[0],\n",
    "        \"artifacts\": [],\n",
    "    }\n",
    "\n",
    "    # Save table\n",
    "    csv_path = out_dir / \"telemetry.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    manifest[\"table_csv\"] = str(csv_path.relative_to(DOCS_DIR)) if csv_path.is_relative_to(DOCS_DIR) else str(csv_path)\n",
    "\n",
    "    # Save figures\n",
    "    for key, fig in figs.items():\n",
    "        png_path = out_dir / f\"{key}.png\"\n",
    "        svg_path = out_dir / f\"{key}.svg\"\n",
    "        fig.savefig(png_path, dpi=160, bbox_inches=\"tight\")\n",
    "        fig.savefig(svg_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        manifest[\"artifacts\"].append({\n",
    "            \"name\": key,\n",
    "            \"png\": str(png_path.relative_to(DOCS_DIR)) if png_path.is_relative_to(DOCS_DIR) else str(png_path),\n",
    "            \"svg\": str(svg_path.relative_to(DOCS_DIR)) if svg_path.is_relative_to(DOCS_DIR) else str(svg_path),\n",
    "            \"caption\": f\"{key.capitalize()} for {manifest['sequence']}\",\n",
    "            \"alt\": f\"{key} plot for sequence {manifest['sequence']}\",\n",
    "        })\n",
    "\n",
    "    # Meta\n",
    "    manifest[\"summary\"] = validate_df(df)\n",
    "    manifest[\"env\"] = ENV_INFO\n",
    "\n",
    "    # Write manifest\n",
    "    manifest_path = out_dir / \"manifest.json\"\n",
    "    manifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "    manifest[\"manifest\"] = str(manifest_path.relative_to(DOCS_DIR)) if manifest_path.is_relative_to(DOCS_DIR) else str(manifest_path)\n",
    "    return manifest\n",
    "\n",
    "\n",
    "# Demo export for first sequence (paths under docs/assets/interactions/<name>)\n",
    "_demo_df = run_sequence(SEQUENCES[0])\n",
    "_demo_figs = plot_telemetry(_demo_df)\n",
    "_demo_out = ASSETS_DIR / SEQUENCES[0].name\n",
    "_demo_manifest = export_sequence(_demo_df, _demo_figs, _demo_out)\n",
    "_demo_manifest\n",
    "\n",
    "## 9) Generar docs/INTERACTIONS_GUIDE.md\n",
    "\n",
    "Escribimos el Markdown con índice de secuencias, imágenes y enlaces a CSV/JSON. Si ya existe, lo sobreescribimos (fuente canónica para esta guía).\n",
    "\n",
    "def write_interactions_guide(manifests: list[dict[str, Any]], guide_path: Path | None = None) -> Path:\n",
    "    guide = guide_path or (DOCS_DIR / \"INTERACTIONS_GUIDE.md\")\n",
    "    lines: list[str] = []\n",
    "    lines.append(\"# Interaction Sequences Guide\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"This page is generated by notebooks/Interaction_Sequences.ipynb. It summarizes canonical interaction sequences, their telemetry, and links to CSV/JSON artifacts.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Sequences\")\n",
    "    for m in manifests:\n",
    "        seq = m[\"sequence\"]\n",
    "        lines.append(f\"### {seq}\")\n",
    "        # Summary table\n",
    "        summ = m.get(\"summary\", {})\n",
    "        lines.append(\"- Rows: \" + str(summ.get(\"rows\", \"?\")))\n",
    "        lines.append(\"- Duration [s]: \" + str(round(summ.get(\"duration_s\", 0.0), 2)))\n",
    "        lines.append(\"- Hash: `\" + str(summ.get(\"hash\", \"\")) + \"`\")\n",
    "        lines.append(\"\")\n",
    "        # Table link\n",
    "        if \"table_csv\" in m:\n",
    "            lines.append(f\"Data (CSV): [{m['table_csv']}]({m['table_csv']})\")\n",
    "            lines.append(\"\")\n",
    "        # Images\n",
    "        for art in m.get(\"artifacts\", []):\n",
    "            png = art.get(\"png\")\n",
    "            caption = art.get(\"caption\", art.get(\"name\", \"figure\"))\n",
    "            alt = art.get(\"alt\", caption)\n",
    "            if png:\n",
    "                lines.append(f\"![{alt}]({png})\")\n",
    "                lines.append(f\"<sub>{caption}</sub>\")\n",
    "                lines.append(\"\")\n",
    "        # Manifest link\n",
    "        if \"manifest\" in m:\n",
    "            lines.append(f\"Manifest (JSON): [{m['manifest']}]({m['manifest']})\")\n",
    "        lines.append(\"\")\n",
    "    # Environment\n",
    "    lines.append(\"## Environment\")\n",
    "    lines.append(\"```json\")\n",
    "    lines.append(json.dumps(ENV_INFO, indent=2))\n",
    "    lines.append(\"```\")\n",
    "\n",
    "    guide.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    return guide\n",
    "\n",
    "\n",
    "# Write guide for demo manifest (full export will add all)\n",
    "_guide_path = write_interactions_guide([_demo_manifest])\n",
    "str(_guide_path)\n",
    "\n",
    "## 10) Tarea de exportación: función CLI y tarea de VS Code\n",
    "\n",
    "Creamos export_all(sequences) que orquesta ejecución y exportación, y un CLI simple. La tarea de VS Code invocará nbconvert para ejecutar este notebook y producir un HTML, mientras que este CLI produce artefactos bajo docs/assets/.\n",
    "\n",
    "def export_all(sequences: list[InteractionSequence] | None = None) -> list[dict[str, Any]]:\n",
    "    seqs = sequences or SEQUENCES\n",
    "    manifests: list[dict[str, Any]] = []\n",
    "    for seq in seqs:\n",
    "        df = run_sequence(seq)\n",
    "        figs = plot_telemetry(df)\n",
    "        out_dir = ASSETS_DIR / seq.name\n",
    "        m = export_sequence(df, figs, out_dir)\n",
    "        manifests.append(m)\n",
    "    write_interactions_guide(manifests)\n",
    "    return manifests\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Export interaction sequences artifacts and guide\")\n",
    "    parser.add_argument(\"--seq\", dest=\"seq\", default=None, help=\"Single sequence name to export\")\n",
    "    parser.add_argument(\"--all\", dest=\"all\", action=\"store_true\", help=\"Export all sequences\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seq:\n",
    "        chosen = [s for s in SEQUENCES if s.name == args.seq]\n",
    "        if not chosen:\n",
    "            raise SystemExit(f\"Sequence not found: {args.seq}\")\n",
    "        export_all(chosen)\n",
    "    else:\n",
    "        export_all(SEQUENCES if args.all else SEQUENCES[:1])\n",
    "\n",
    "## 11) Escribir pruebas mínimas y ejecutar en VS Code\n",
    "\n",
    "Generamos un archivo de pruebas mínimo para validar invariantes. En VS Code, puedes ejecutar las pruebas con el Test Explorer o desde la terminal.\n",
    "\n",
    "TESTS_DIR = REPO_ROOT / \"tests\"\n",
    "TESTS_DIR.mkdir(exist_ok=True)\n",
    "TEST_FILE = TESTS_DIR / \"test_interactions_sequences_nb.py\"\n",
    "TEST_FILE.write_text(\n",
    "    \"\"\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def test_manifest_and_csv_exist():\n",
    "    guide = Path('docs/INTERACTIONS_GUIDE.md')\n",
    "    assert guide.exists(), 'Guide must exist after running the notebook export cells.'\n",
    "    # Find at least one manifest and CSV under assets\n",
    "    assets = Path('docs/assets/interactions')\n",
    "    assert assets.exists(), 'Assets dir must exist.'\n",
    "    found_manifest = False\n",
    "    found_csv = False\n",
    "    for p in assets.rglob('manifest.json'):\n",
    "        found_manifest = True\n",
    "    for p in assets.rglob('telemetry.csv'):\n",
    "        df = pd.read_csv(p)\n",
    "        assert not df.empty\n",
    "        found_csv = True\n",
    "    assert found_manifest, 'At least one manifest.json must be exported.'\n",
    "    assert found_csv, 'At least one telemetry.csv must be exported.'\n",
    "    \"\"\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"Wrote test file: {TEST_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90415899",
   "metadata": {},
   "source": [
    "## 1) Configurar entorno y rutas de proyecto\n",
    "\n",
    "Detectamos la raíz del repositorio (git o heurística con pathlib), creamos carpetas de salida y parametrizamos rutas para activos (PNG/SVG/CSV/JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a36e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\TNFR-Python-Engine\n",
      "Assets dir: C:\\TNFR-Python-Engine\\docs\\assets\\interactions\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Detect repo root via git, else fallback to current working directory\n",
    "try:\n",
    "    import subprocess\n",
    "    root_guess = Path.cwd()\n",
    "    git_root = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], cwd=root_guess).decode().strip()\n",
    "    REPO_ROOT = Path(git_root)\n",
    "except Exception:\n",
    "    # Heurística: buscar pyproject.toml hacia arriba\n",
    "    p = Path.cwd()\n",
    "    while p != p.parent:\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            REPO_ROOT = p\n",
    "            break\n",
    "        p = p.parent\n",
    "    else:\n",
    "        REPO_ROOT = Path.cwd()\n",
    "\n",
    "# Ensure src is importable during interactive runs\n",
    "SRC_DIR = REPO_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "DOCS_DIR = REPO_ROOT / \"docs\"\n",
    "ASSETS_DIR = DOCS_DIR / \"assets\" / \"interactions\"\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "REPORTS_DIR = REPO_ROOT / \"results\" / \"reports\"\n",
    "\n",
    "for d in [DOCS_DIR, ASSETS_DIR, DATA_DIR, REPORTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Assets dir: {ASSETS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40e5d6",
   "metadata": {},
   "source": [
    "## 2) Importar librerías y establecer semilla\n",
    "\n",
    "Importamos librerías mínimas y fijamos semilla global para reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dddf5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nuevo\\AppData\\Local\\Temp\\ipykernel_26752\\4252616425.py:17: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'numpy_seed': 1337}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "import math\n",
    "import hashlib\n",
    "import platform\n",
    "import random\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    # Optional: richer version reporting\n",
    "    import pkg_resources  # type: ignore\n",
    "except Exception:\n",
    "    pkg_resources = None\n",
    "\n",
    "# Fixed seeds for reproducibility\n",
    "SEED = int(os.environ.get(\"INTERACTIONS_SEED\", 1337))\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print({\"numpy_seed\": SEED})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae51af",
   "metadata": {},
   "source": [
    "## 3) Cargar o simular telemetría base\n",
    "\n",
    "Implementamos carga desde CSV/JSON en data/ y un simulador de series sintéticas (tiempo, latencia, throughput, errores) con ruido controlado por la semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4f75b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nuevo\\AppData\\Local\\Temp\\ipykernel_26752\\2291027102.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  start_ts = datetime.utcnow()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>t_s</th>\n",
       "      <th>step</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>throughput_rps</th>\n",
       "      <th>error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-12 14:34:17.822803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>smoke</td>\n",
       "      <td>115.431990</td>\n",
       "      <td>45.512227</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-12 14:34:23.945252</td>\n",
       "      <td>6.122449</td>\n",
       "      <td>smoke</td>\n",
       "      <td>122.828537</td>\n",
       "      <td>48.413510</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-12 14:34:30.067701</td>\n",
       "      <td>12.244898</td>\n",
       "      <td>smoke</td>\n",
       "      <td>117.161813</td>\n",
       "      <td>50.533080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-12 14:34:36.190150</td>\n",
       "      <td>18.367347</td>\n",
       "      <td>smoke</td>\n",
       "      <td>115.127453</td>\n",
       "      <td>50.299730</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-12 14:34:42.312599</td>\n",
       "      <td>24.489796</td>\n",
       "      <td>smoke</td>\n",
       "      <td>110.435493</td>\n",
       "      <td>55.100262</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ts        t_s   step  latency_ms  throughput_rps  \\\n",
       "0 2025-11-12 14:34:17.822803   0.000000  smoke  115.431990       45.512227   \n",
       "1 2025-11-12 14:34:23.945252   6.122449  smoke  122.828537       48.413510   \n",
       "2 2025-11-12 14:34:30.067701  12.244898  smoke  117.161813       50.533080   \n",
       "3 2025-11-12 14:34:36.190150  18.367347  smoke  115.127453       50.299730   \n",
       "4 2025-11-12 14:34:42.312599  24.489796  smoke  110.435493       55.100262   \n",
       "\n",
       "   error_rate  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_telemetry(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    if path.suffix.lower() in {\".json\", \".jsonl\"}:\n",
    "        if path.suffix.lower() == \".jsonl\":\n",
    "            rows = [json.loads(line) for line in path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "            return pd.DataFrame(rows)\n",
    "        return pd.DataFrame(json.loads(path.read_text(encoding=\"utf-8\")))\n",
    "    raise ValueError(f\"Unsupported telemetry format: {path.suffix}\")\n",
    "\n",
    "\n",
    "def simulate_telemetry(n: int = 300, base_latency_ms: float = 120.0, base_rps: float = 50.0,\n",
    "                        error_p: float = 0.01, spike_every: int | None = 60,\n",
    "                        duration_s: int = 300, step_label: str = \"base\",\n",
    "                        start_ts: datetime | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Generate synthetic telemetry with gentle trends and occasional spikes.\n",
    "    Columns: ts, t_s, step, latency_ms, throughput_rps, error_rate.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(SEED + hash(step_label) % 10000)\n",
    "    if start_ts is None:\n",
    "        start_ts = datetime.utcnow()\n",
    "    times = np.linspace(0, duration_s, n)\n",
    "    ts = [start_ts + timedelta(seconds=float(t)) for t in times]\n",
    "\n",
    "    # Latency with low-frequency drift and noise\n",
    "    drift = 0.2 * np.sin(2 * np.pi * times / max(1.0, duration_s / 5))\n",
    "    noise = rng.normal(0, 5.0, size=n)\n",
    "    latency = np.clip(base_latency_ms * (1.0 + 0.01 * drift) + noise, 1.0, None)\n",
    "\n",
    "    # Throughput with mild trend and anti-correlation to latency\n",
    "    throughput = np.clip(base_rps * (1.0 - 0.002 * drift) + rng.normal(0, 2.0, size=n), 0.0, None)\n",
    "\n",
    "    # Error rate baseline with rare spikes\n",
    "    errors = np.clip(rng.binomial(1, error_p, size=n) * rng.uniform(0.2, 0.8, size=n), 0.0, 1.0)\n",
    "\n",
    "    if spike_every:\n",
    "        for i in range(n):\n",
    "            if i % spike_every == 0 and i > 0:\n",
    "                latency[i] *= rng.uniform(1.2, 1.6)\n",
    "                throughput[i] *= rng.uniform(0.6, 0.9)\n",
    "                errors[i] = max(errors[i], rng.uniform(0.05, 0.2))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"ts\": ts,\n",
    "        \"t_s\": times,\n",
    "        \"step\": step_label,\n",
    "        \"latency_ms\": latency,\n",
    "        \"throughput_rps\": throughput,\n",
    "        \"error_rate\": errors,\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "# quick smoke\n",
    "_sim = simulate_telemetry(n=50, step_label=\"smoke\")\n",
    "_sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb668eb6",
   "metadata": {},
   "source": [
    "## 4) Definir secuencias de interacción canónicas\n",
    "\n",
    "Creamos dataclasses y un mini-DSL YAML/Dict para describir pasos (acción, duración, parámetros). Incluimos casos canónicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb7afae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['login-browse-purchase', 'search-filter-paginate', 'retry-on-error']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class InteractionStep:\n",
    "    action: str\n",
    "    duration_s: int\n",
    "    params: Dict[str, Any]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InteractionSequence:\n",
    "    name: str\n",
    "    steps: list[InteractionStep]\n",
    "\n",
    "\n",
    "def parse_sequences_from_yaml(yaml_text: str) -> list[InteractionSequence]:\n",
    "    doc = yaml.safe_load(yaml_text)\n",
    "    sequences: list[InteractionSequence] = []\n",
    "    for entry in doc.get(\"sequences\", []):\n",
    "        steps = [InteractionStep(s[\"action\"], int(s.get(\"duration_s\", 60)), s.get(\"params\", {})) for s in entry[\"steps\"]]\n",
    "        sequences.append(InteractionSequence(name=entry[\"name\"], steps=steps))\n",
    "    return sequences\n",
    "\n",
    "\n",
    "CANONICAL_YAML = \"\"\"\n",
    "sequences:\n",
    "  - name: login-browse-purchase\n",
    "    steps:\n",
    "      - action: login\n",
    "        duration_s: 60\n",
    "        params: { base_latency_ms: 180, base_rps: 30, error_p: 0.02 }\n",
    "      - action: browse\n",
    "        duration_s: 120\n",
    "        params: { base_latency_ms: 120, base_rps: 60, error_p: 0.01 }\n",
    "      - action: purchase\n",
    "        duration_s: 90\n",
    "        params: { base_latency_ms: 150, base_rps: 40, error_p: 0.03 }\n",
    "  - name: search-filter-paginate\n",
    "    steps:\n",
    "      - action: search\n",
    "        duration_s: 100\n",
    "        params: { base_latency_ms: 140, base_rps: 55, error_p: 0.015 }\n",
    "      - action: filter\n",
    "        duration_s: 80\n",
    "        params: { base_latency_ms: 160, base_rps: 45, error_p: 0.02 }\n",
    "      - action: paginate\n",
    "        duration_s: 140\n",
    "        params: { base_latency_ms: 130, base_rps: 65, error_p: 0.012 }\n",
    "  - name: retry-on-error\n",
    "    steps:\n",
    "      - action: call\n",
    "        duration_s: 60\n",
    "        params: { base_latency_ms: 110, base_rps: 70, error_p: 0.005 }\n",
    "      - action: error\n",
    "        duration_s: 30\n",
    "        params: { base_latency_ms: 220, base_rps: 20, error_p: 0.15, spike_every: 10 }\n",
    "      - action: retry\n",
    "        duration_s: 60\n",
    "        params: { base_latency_ms: 130, base_rps: 55, error_p: 0.02 }\n",
    "\"\"\"\n",
    "\n",
    "SEQUENCES = parse_sequences_from_yaml(CANONICAL_YAML)\n",
    "[name for name in [s.name for s in SEQUENCES]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad4f85",
   "metadata": {},
   "source": [
    "## 5) Ejecutar secuencias y registrar telemetría\n",
    "\n",
    "Ejecutamos cada paso contra el simulador y agregamos DataFrames con etiquetas y timestamps continuos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3bf4be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nuevo\\AppData\\Local\\Temp\\ipykernel_26752\\1488497.py:3: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  cur_ts = start_ts or datetime.utcnow()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>t_s</th>\n",
       "      <th>step</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>throughput_rps</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-12 14:34:29.494867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>login</td>\n",
       "      <td>178.172871</td>\n",
       "      <td>31.055583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>login-browse-purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-12 14:34:30.511816</td>\n",
       "      <td>1.016949</td>\n",
       "      <td>login</td>\n",
       "      <td>180.243478</td>\n",
       "      <td>29.782623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>login-browse-purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-12 14:34:31.528765</td>\n",
       "      <td>2.033898</td>\n",
       "      <td>login</td>\n",
       "      <td>185.146571</td>\n",
       "      <td>30.964893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>login-browse-purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-12 14:34:32.545714</td>\n",
       "      <td>3.050847</td>\n",
       "      <td>login</td>\n",
       "      <td>188.108166</td>\n",
       "      <td>30.398561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>login-browse-purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-12 14:34:33.562664</td>\n",
       "      <td>4.067797</td>\n",
       "      <td>login</td>\n",
       "      <td>183.744614</td>\n",
       "      <td>29.335008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>login-browse-purchase</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ts       t_s   step  latency_ms  throughput_rps  \\\n",
       "0 2025-11-12 14:34:29.494867  0.000000  login  178.172871       31.055583   \n",
       "1 2025-11-12 14:34:30.511816  1.016949  login  180.243478       29.782623   \n",
       "2 2025-11-12 14:34:31.528765  2.033898  login  185.146571       30.964893   \n",
       "3 2025-11-12 14:34:32.545714  3.050847  login  188.108166       30.398561   \n",
       "4 2025-11-12 14:34:33.562664  4.067797  login  183.744614       29.335008   \n",
       "\n",
       "   error_rate               sequence  \n",
       "0         0.0  login-browse-purchase  \n",
       "1         0.0  login-browse-purchase  \n",
       "2         0.0  login-browse-purchase  \n",
       "3         0.0  login-browse-purchase  \n",
       "4         0.0  login-browse-purchase  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_sequence(seq: InteractionSequence, start_ts: datetime | None = None) -> pd.DataFrame:\n",
    "    frames: list[pd.DataFrame] = []\n",
    "    cur_ts = start_ts or datetime.utcnow()\n",
    "    t_offset = 0.0\n",
    "    for step in seq.steps:\n",
    "        params = dict(step.params)\n",
    "        params.setdefault(\"duration_s\", step.duration_s)\n",
    "        df = simulate_telemetry(\n",
    "            n=max(50, int(params[\"duration_s\"]) // 1),\n",
    "            base_latency_ms=float(params.get(\"base_latency_ms\", 120.0)),\n",
    "            base_rps=float(params.get(\"base_rps\", 50.0)),\n",
    "            error_p=float(params.get(\"error_p\", 0.01)),\n",
    "            spike_every=params.get(\"spike_every\"),\n",
    "            duration_s=int(params[\"duration_s\"]),\n",
    "            step_label=step.action,\n",
    "            start_ts=cur_ts,\n",
    "        )\n",
    "        # Rebase t_s to cumulative timeline\n",
    "        df[\"t_s\"] = df[\"t_s\"] + t_offset\n",
    "        frames.append(df)\n",
    "        # Advance current ts and offset\n",
    "        cur_ts = df[\"ts\"].iloc[-1]\n",
    "        t_offset = float(df[\"t_s\"].iloc[-1])\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    out[\"sequence\"] = seq.name\n",
    "    return out\n",
    "\n",
    "\n",
    "# Example run\n",
    "_df = run_sequence(SEQUENCES[0])\n",
    "_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ef55b",
   "metadata": {},
   "source": [
    "## 6) Graficar figuras/curvas de telemetría\n",
    "\n",
    "Generamos curvas de latencia, throughput y tasa de errores; devolvemos objetos matplotlib Figure para exportación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f60d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_telemetry(df: pd.DataFrame, title: str = \"\") -> dict[str, plt.Figure]:\n",
    "    figs: dict[str, plt.Figure] = {}\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Latency over time\n",
    "    fig1, ax1 = plt.subplots(figsize=(9, 4))\n",
    "    for step, g in df.groupby(\"step\"):\n",
    "        ax1.plot(g[\"t_s\"], g[\"latency_ms\"], label=step, alpha=0.9)\n",
    "    ax1.set_title(title or f\"Latency over time — {df['sequence'].iloc[0]}\")\n",
    "    ax1.set_xlabel(\"t [s]\")\n",
    "    ax1.set_ylabel(\"latency [ms]\")\n",
    "    ax1.legend()\n",
    "    figs[\"latency\"] = fig1\n",
    "\n",
    "    # Throughput over time\n",
    "    fig2, ax2 = plt.subplots(figsize=(9, 4))\n",
    "    for step, g in df.groupby(\"step\"):\n",
    "        ax2.plot(g[\"t_s\"], g[\"throughput_rps\"], label=step, alpha=0.9)\n",
    "    ax2.set_title(title or f\"Throughput over time — {df['sequence'].iloc[0]}\")\n",
    "    ax2.set_xlabel(\"t [s]\")\n",
    "    ax2.set_ylabel(\"throughput [rps]\")\n",
    "    ax2.legend()\n",
    "    figs[\"throughput\"] = fig2\n",
    "\n",
    "    # Error rate over time\n",
    "    fig3, ax3 = plt.subplots(figsize=(9, 4))\n",
    "    for step, g in df.groupby(\"step\"):\n",
    "        ax3.plot(g[\"t_s\"], g[\"error_rate\"], label=step, alpha=0.9)\n",
    "    ax3.set_title(title or f\"Error rate over time — {df['sequence'].iloc[0]}\")\n",
    "    ax3.set_xlabel(\"t [s]\")\n",
    "    ax3.set_ylabel(\"error rate [0..1]\")\n",
    "    ax3.legend()\n",
    "    figs[\"errors\"] = fig3\n",
    "\n",
    "    # Distributions\n",
    "    fig4, ax4 = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    sns.kdeplot(df[\"latency_ms\"], fill=True, ax=ax4[0])\n",
    "    ax4[0].set_title(\"Latency dist\")\n",
    "    sns.kdeplot(df[\"throughput_rps\"], fill=True, ax=ax4[1])\n",
    "    ax4[1].set_title(\"Throughput dist\")\n",
    "    sns.kdeplot(df[\"error_rate\"], fill=True, ax=ax4[2])\n",
    "    ax4[2].set_title(\"Error rate dist\")\n",
    "    fig4.suptitle(title or f\"Distributions — {df['sequence'].iloc[0]}\")\n",
    "    figs[\"distributions\"] = fig4\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return figs\n",
    "\n",
    "\n",
    "_figs = plot_telemetry(_df)\n",
    "plt.close('all')  # Avoid duplicate inline rendering when exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b2cbe7",
   "metadata": {},
   "source": [
    "## 7) Validar resultados y reproducibilidad\n",
    "\n",
    "Calculamos hashes de parámetros/datos, registramos versión de dependencias y semilla, y afirmamos invariantes básicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d5cb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': '3.13.6',\n",
       " 'platform': 'Windows-11-10.0.26200-SP0',\n",
       " 'seed': 1337,\n",
       " 'numpy': '2.3.4',\n",
       " 'pandas': '2.2.3',\n",
       " 'matplotlib': '3.10.7',\n",
       " 'seaborn': '0.13.2',\n",
       " 'pyyaml': '6.0.2'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_hash(df: pd.DataFrame) -> str:\n",
    "    m = hashlib.sha256()\n",
    "    # Stable hash: round floats; serialize selected columns\n",
    "    cols = [\"t_s\", \"latency_ms\", \"throughput_rps\", \"error_rate\", \"step\", \"sequence\"]\n",
    "    sub = df[cols].copy()\n",
    "    for c in [\"t_s\", \"latency_ms\", \"throughput_rps\", \"error_rate\"]:\n",
    "        sub[c] = pd.to_numeric(sub[c], errors=\"coerce\").round(6)\n",
    "    payload = sub.to_csv(index=False).encode(\"utf-8\")\n",
    "    m.update(payload)\n",
    "    return m.hexdigest()\n",
    "\n",
    "\n",
    "def validate_df(df: pd.DataFrame) -> dict[str, Any]:\n",
    "    assert (df[\"t_s\"].diff().fillna(0) >= 0).all(), \"Timestamps must be non-decreasing\"\n",
    "    for c in [\"latency_ms\", \"throughput_rps\", \"error_rate\"]:\n",
    "        assert (df[c] >= 0).all(), f\"{c} must be non-negative\"\n",
    "    return {\n",
    "        \"rows\": int(len(df)),\n",
    "        \"duration_s\": float(df[\"t_s\"].iloc[-1] - df[\"t_s\"].iloc[0]) if len(df) > 1 else 0.0,\n",
    "        \"hash\": df_hash(df),\n",
    "    }\n",
    "\n",
    "\n",
    "ENV_INFO = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "if pkg_resources is not None:\n",
    "    try:\n",
    "        ENV_INFO[\"numpy\"] = pkg_resources.get_distribution(\"numpy\").version\n",
    "        ENV_INFO[\"pandas\"] = pkg_resources.get_distribution(\"pandas\").version\n",
    "        ENV_INFO[\"matplotlib\"] = pkg_resources.get_distribution(\"matplotlib\").version\n",
    "        ENV_INFO[\"seaborn\"] = pkg_resources.get_distribution(\"seaborn\").version\n",
    "        ENV_INFO[\"pyyaml\"] = pkg_resources.get_distribution(\"pyyaml\").version\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "ENV_INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792a863",
   "metadata": {},
   "source": [
    "## 8) Exportar figuras y artefactos a docs/assets/\n",
    "\n",
    "Guardamos figuras (PNG/SVG), tablas (CSV) y un manifiesto con rutas y metadatos para cada secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6640acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nuevo\\AppData\\Local\\Temp\\ipykernel_26752\\1488497.py:3: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  cur_ts = start_ts or datetime.utcnow()\n",
      "C:\\Users\\nuevo\\AppData\\Local\\Temp\\ipykernel_26752\\1488497.py:3: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  cur_ts = start_ts or datetime.utcnow()\n",
      "C:\\Users\\nuevo\\AppData\\Local\\Temp\\ipykernel_26752\\1488497.py:3: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  cur_ts = start_ts or datetime.utcnow()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'login-browse-purchase', 'rows': 270},\n",
       " {'sequence': 'search-filter-paginate', 'rows': 320},\n",
       " {'sequence': 'retry-on-error', 'rows': 170}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def export_sequence(df: pd.DataFrame, figs: dict[str, plt.Figure], out_dir: Path) -> dict[str, Any]:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    manifest: dict[str, Any] = {\n",
    "        \"sequence\": df[\"sequence\"].iloc[0],\n",
    "        \"artifacts\": [],\n",
    "    }\n",
    "\n",
    "    # Save table\n",
    "    csv_path = out_dir / \"telemetry.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    try:\n",
    "        rel_csv = csv_path.relative_to(DOCS_DIR)\n",
    "        manifest[\"table_csv\"] = str(rel_csv)\n",
    "    except ValueError:\n",
    "        manifest[\"table_csv\"] = str(csv_path)\n",
    "\n",
    "    # Save figures\n",
    "    for key, fig in figs.items():\n",
    "        png_path = out_dir / f\"{key}.png\"\n",
    "        svg_path = out_dir / f\"{key}.svg\"\n",
    "        fig.savefig(png_path, dpi=160, bbox_inches=\"tight\")\n",
    "        fig.savefig(svg_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        try:\n",
    "            rel_png = png_path.relative_to(DOCS_DIR)\n",
    "            rel_svg = svg_path.relative_to(DOCS_DIR)\n",
    "        except ValueError:\n",
    "            rel_png = png_path\n",
    "            rel_svg = svg_path\n",
    "        manifest[\"artifacts\"].append({\n",
    "            \"name\": key,\n",
    "            \"png\": str(rel_png),\n",
    "            \"svg\": str(rel_svg),\n",
    "            \"caption\": f\"{key.capitalize()} for {manifest['sequence']}\",\n",
    "            \"alt\": f\"{key} plot for sequence {manifest['sequence']}\",\n",
    "        })\n",
    "\n",
    "    # Meta\n",
    "    manifest[\"summary\"] = validate_df(df)\n",
    "    manifest[\"env\"] = ENV_INFO\n",
    "\n",
    "    # Write manifest\n",
    "    manifest_path = out_dir / \"manifest.json\"\n",
    "    manifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "    try:\n",
    "        rel_manifest = manifest_path.relative_to(DOCS_DIR)\n",
    "        manifest[\"manifest\"] = str(rel_manifest)\n",
    "    except ValueError:\n",
    "        manifest[\"manifest\"] = str(manifest_path)\n",
    "    return manifest\n",
    "\n",
    "\n",
    "# Export ALL sequences to docs/assets/interactions/<sequence>/ and build manifests\n",
    "manifests: list[dict[str, Any]] = []\n",
    "for _seq in SEQUENCES:\n",
    "    _df = run_sequence(_seq)\n",
    "    _figs = plot_telemetry(_df)\n",
    "    _out = ASSETS_DIR / _seq.name\n",
    "    _m = export_sequence(_df, _figs, _out)\n",
    "    manifests.append(_m)\n",
    "\n",
    "# Show a compact summary (sequence + rows)\n",
    "[{\"sequence\": m.get(\"sequence\"), \"rows\": m.get(\"summary\", {}).get(\"rows\")} for m in manifests]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbaa936",
   "metadata": {},
   "source": [
    "## 9) Generar docs/INTERACTIONS_GUIDE.md\n",
    "\n",
    "Escribimos el Markdown con índice de secuencias, imágenes y enlaces a CSV/JSON. Si ya existe, se sobreescribe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d38e54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\TNFR-Python-Engine\\\\docs\\\\INTERACTIONS_GUIDE.md'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_interactions_guide(manifests: list[dict[str, Any]], guide_path: Path | None = None) -> Path:\n",
    "    guide = guide_path or (DOCS_DIR / \"INTERACTIONS_GUIDE.md\")\n",
    "    lines: list[str] = []\n",
    "    lines.append(\"# Interaction Sequences Guide\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"This page is generated by notebooks/Interaction_Sequences.ipynb. It summarizes canonical interaction sequences, their telemetry, and links to CSV/JSON artifacts.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Sequences\")\n",
    "    for m in manifests:\n",
    "        seq = m[\"sequence\"]\n",
    "        lines.append(f\"### {seq}\")\n",
    "        summ = m.get(\"summary\", {})\n",
    "        lines.append(\"- Rows: \" + str(summ.get(\"rows\", \"?\")))\n",
    "        lines.append(\"- Duration [s]: \" + str(round(summ.get(\"duration_s\", 0.0), 2)))\n",
    "        lines.append(\"- Hash: `\" + str(summ.get(\"hash\", \"\")) + \"`\")\n",
    "        lines.append(\"\")\n",
    "        if \"table_csv\" in m:\n",
    "            lines.append(f\"Data (CSV): [{m['table_csv']}]({m['table_csv']})\")\n",
    "            lines.append(\"\")\n",
    "        for art in m.get(\"artifacts\", []):\n",
    "            png = art.get(\"png\")\n",
    "            caption = art.get(\"caption\", art.get(\"name\", \"figure\"))\n",
    "            alt = art.get(\"alt\", caption)\n",
    "            if png:\n",
    "                lines.append(f\"![{alt}]({png})\")\n",
    "                lines.append(f\"<sub>{caption}</sub>\")\n",
    "                lines.append(\"\")\n",
    "        if \"manifest\" in m:\n",
    "            lines.append(f\"Manifest (JSON): [{m['manifest']}]({m['manifest']})\")\n",
    "        lines.append(\"\")\n",
    "    lines.append(\"## Environment\")\n",
    "    lines.append(\"```json\")\n",
    "    lines.append(json.dumps(ENV_INFO, indent=2))\n",
    "    lines.append(\"```\")\n",
    "\n",
    "    guide.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    return guide\n",
    "\n",
    "\n",
    "# Write guide for all exported manifests\n",
    "_guide_path = write_interactions_guide(manifests)\n",
    "str(_guide_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b008a0",
   "metadata": {},
   "source": [
    "## 10) Tarea de exportación: función CLI y tarea de VS Code\n",
    "\n",
    "Creamos export_all(sequences) que orquesta ejecución y exportación, y un CLI simple. La tarea de VS Code invocará nbconvert para ejecutar este notebook y producir un HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--seq SEQ] [--all]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\nuevo\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3a1425fe1cd0da960601d316af915ea2f5a97153a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nuevo\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def export_all(sequences: list[InteractionSequence] | None = None) -> list[dict[str, Any]]:\n",
    "    seqs = sequences or SEQUENCES\n",
    "    manifests: list[dict[str, Any]] = []\n",
    "    for seq in seqs:\n",
    "        df = run_sequence(seq)\n",
    "        figs = plot_telemetry(df)\n",
    "        out_dir = ASSETS_DIR / seq.name\n",
    "        m = export_sequence(df, figs, out_dir)\n",
    "        manifests.append(m)\n",
    "    write_interactions_guide(manifests)\n",
    "    return manifests\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse, sys\n",
    "    parser = argparse.ArgumentParser(description=\"Export interaction sequences artifacts and guide\")\n",
    "    parser.add_argument(\"--seq\", dest=\"seq\", default=None, help=\"Single sequence name to export\")\n",
    "    parser.add_argument(\"--all\", dest=\"all\", action=\"store_true\", help=\"Export all sequences\")\n",
    "    # In Jupyter/IPython, unknown args are present; use parse_known_args to avoid SystemExit\n",
    "    args, _unknown = parser.parse_known_args()\n",
    "\n",
    "    if args.seq:\n",
    "        chosen = [s for s in SEQUENCES if s.name == args.seq]\n",
    "        if not chosen:\n",
    "            raise SystemExit(f\"Sequence not found: {args.seq}\")\n",
    "        export_all(chosen)\n",
    "    else:\n",
    "        export_all(SEQUENCES if args.all else SEQUENCES[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c234d8",
   "metadata": {},
   "source": [
    "## 11) Escribir pruebas mínimas y ejecutar en VS Code\n",
    "\n",
    "Generamos un archivo de pruebas mínimo para validar invariantes. En VS Code, puedes ejecutar las pruebas con el Test Explorer o desde la terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df605e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote test file: C:\\TNFR-Python-Engine\\tests\\test_interactions_sequences_nb.py\n"
     ]
    }
   ],
   "source": [
    "TESTS_DIR = REPO_ROOT / \"tests\"\n",
    "TESTS_DIR.mkdir(exist_ok=True)\n",
    "TEST_FILE = TESTS_DIR / \"test_interactions_sequences_nb.py\"\n",
    "TEST_FILE.write_text(\n",
    "    \"\"\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def test_manifest_and_csv_exist():\n",
    "    guide = Path('docs/INTERACTIONS_GUIDE.md')\n",
    "    assert guide.exists(), 'Guide must exist after running the notebook export cells.'\n",
    "    # Find at least one manifest and CSV under assets\n",
    "    assets = Path('docs/assets/interactions')\n",
    "    assert assets.exists(), 'Assets dir must exist.'\n",
    "    found_manifest = False\n",
    "    found_csv = False\n",
    "    for p in assets.rglob('manifest.json'):\n",
    "        found_manifest = True\n",
    "    for p in assets.rglob('telemetry.csv'):\n",
    "        df = pd.read_csv(p)\n",
    "        assert not df.empty\n",
    "        found_csv = True\n",
    "    assert found_manifest, 'At least one manifest.json must be exported.'\n",
    "    assert found_csv, 'At least one telemetry.csv must be exported.'\n",
    "    \"\"\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"Wrote test file: {TEST_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
