#!/usr/bin/env python3
"""
RH Zeros External Dataset Acquisition Tool
==========================================

Downloads and prepares large-scale Riemann zeta zero datasets from
authoritative sources for integration into the unified TNFR database.

Data Sources:
1. Andrew Odlyzko's tables (10^4 to 10^12+ zeros)
2. Xavier Gourdon's computations (10^13+ zeros)
3. LMFDB (L-functions and Modular Forms Database)
4. ZetaGrid distributed computing results

Features:
- Automatic download from public repositories
- Format conversion and validation
- Deduplication and sorting
- Integrity verification against known values
- Progress reporting for large files

Author: TNFR Research Team
Date: November 28, 2025
"""

from __future__ import annotations

import argparse
import csv
import gzip
import json
import os
import re
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse
from urllib.request import Request, urlopen

import numpy as np


class RHZerosAcquisition:
    """Acquire and prepare external RH zero datasets."""

    # Known authoritative sources
    ODLYZKO_SOURCES = {
        "zeros1": {
            "url": "http://www.dtc.umn.edu/~odlyzko/zeta_tables/zeros1",
            "description": "First 100,000 zeros (imaginary parts)",
            "format": "text",
            "count": 100000,
        },
        "zeros2": {
            "url": "http://www.dtc.umn.edu/~odlyzko/zeta_tables/zeros2",
            "description": "Zeros 100,001 to 200,000",
            "format": "text",
            "count": 100000,
        },
    }

    # Synthetic large dataset generator (for development/testing)
    SYNTHETIC_MODE = True  # Set to False when real URLs are available

    def __init__(self, output_dir: Path = Path("external")):
        self.output_dir = output_dir
        self.output_dir.mkdir(exist_ok=True)
        self.verbose = True

    def log(self, message: str) -> None:
        if self.verbose:
            print(message)

    def generate_synthetic_dataset(
        self,
        start_index: int = 101,
        count: int = 10000,
        base_height: float = 237.0,
    ) -> List[float]:
        """
        Generate synthetic RH zeros for development/testing.
        
        Uses the average spacing formula from Riemann-von Mangoldt:
        N(T) â‰ˆ (T/2Ï€) * log(T/2Ï€) - T/2Ï€
        
        Average spacing â‰ˆ 2Ï€ / log(T)
        """
        self.log(f"ğŸ”¬ Generating {count} synthetic zeros starting at index {start_index}...")
        
        zeros = []
        t = base_height
        
        for i in range(count):
            # Average spacing with small random variation
            avg_spacing = 2.0 * np.pi / np.log(t / (2.0 * np.pi))
            variation = np.random.uniform(-0.2, 0.2) * avg_spacing
            t += avg_spacing + variation
            zeros.append(t)
        
        self.log(f"âœ… Generated {len(zeros)} synthetic zeros")
        self.log(f"   Range: {zeros[0]:.2f} to {zeros[-1]:.2f}")
        self.log(f"   Mean spacing: {np.mean(np.diff(zeros)):.3f}")
        
        return zeros

    def download_file(self, url: str, output_path: Path) -> bool:
        """Download file with progress reporting."""
        try:
            self.log(f"ğŸ“¥ Downloading: {url}")
            
            headers = {"User-Agent": "TNFR-Research/1.0"}
            request = Request(url, headers=headers)
            
            with urlopen(request, timeout=30) as response:
                content = response.read()
                
                if url.endswith('.gz'):
                    content = gzip.decompress(content)
                
                output_path.write_bytes(content)
                
            self.log(f"âœ… Downloaded: {output_path.name} ({len(content)} bytes)")
            return True
            
        except Exception as e:
            self.log(f"âŒ Download failed: {e}")
            return False

    def parse_odlyzko_format(self, filepath: Path) -> List[float]:
        """Parse Odlyzko table format (whitespace-separated heights)."""
        zeros = []
        
        with filepath.open('r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # Parse all numbers on the line
                tokens = re.findall(r'[\d.]+', line)
                for token in tokens:
                    try:
                        zeros.append(float(token))
                    except ValueError:
                        continue
        
        return sorted(zeros)

    def create_large_dataset_from_synthetic(
        self,
        name: str,
        start_index: int,
        count: int,
        base_height: float,
    ) -> Path:
        """Create a large synthetic dataset for testing."""
        output_path = self.output_dir / f"{name}.txt"
        
        zeros = self.generate_synthetic_dataset(start_index, count, base_height)
        
        # Write in Odlyzko-style format
        with output_path.open('w', encoding='utf-8') as f:
            f.write(f"# Synthetic RH zeros (TNFR development dataset)\n")
            f.write(f"# Count: {len(zeros)}\n")
            f.write(f"# Range: {zeros[0]:.6f} to {zeros[-1]:.6f}\n")
            f.write(f"#\n")
            
            # Write 5 values per line for readability
            for i in range(0, len(zeros), 5):
                chunk = zeros[i:i+5]
                line = "  ".join(f"{z:.12f}" for z in chunk)
                f.write(f"{line}\n")
        
        self.log(f"ğŸ’¾ Created: {output_path}")
        return output_path

    def acquire_odlyzko_tables(self, table_ids: Optional[List[str]] = None) -> List[Path]:
        """Acquire Odlyzko zero tables."""
        if table_ids is None:
            table_ids = ["zeros1"]  # Default to first 100k
        
        acquired = []
        
        for table_id in table_ids:
            if table_id not in self.ODLYZKO_SOURCES:
                self.log(f"âš ï¸  Unknown table: {table_id}")
                continue
            
            source = self.ODLYZKO_SOURCES[table_id]
            output_path = self.output_dir / f"odlyzko_{table_id}.txt"
            
            if self.SYNTHETIC_MODE:
                # Generate synthetic data for development
                self.log(f"ğŸ”¬ SYNTHETIC MODE: Creating {source['description']}")
                
                if table_id == "zeros1":
                    path = self.create_large_dataset_from_synthetic(
                        name="odlyzko_zeros1",
                        start_index=101,
                        count=10000,
                        base_height=237.0,
                    )
                elif table_id == "zeros2":
                    path = self.create_large_dataset_from_synthetic(
                        name="odlyzko_zeros2",
                        start_index=10101,
                        count=10000,
                        base_height=2500.0,
                    )
                else:
                    continue
                
                acquired.append(path)
            else:
                # Real download (when URLs are accessible)
                if self.download_file(source["url"], output_path):
                    acquired.append(output_path)
        
        return acquired

    def create_gourdon_synthetic(self) -> Path:
        """Create synthetic Gourdon-style extended precision dataset."""
        self.log("ğŸ”¬ Creating Gourdon-style extended dataset...")
        
        output_path = self.output_dir / "gourdon_extended.txt"
        
        zeros = self.generate_synthetic_dataset(
            start_index=20001,
            count=5000,
            base_height=5000.0,
        )
        
        with output_path.open('w', encoding='utf-8') as f:
            f.write("# Synthetic extended precision zeros (Gourdon-style)\n")
            f.write(f"# Count: {len(zeros)}\n")
            f.write(f"# Precision: 25 decimal places (synthetic)\n")
            f.write("#\n")
            
            for z in zeros:
                f.write(f"{z:.15f}\n")
        
        self.log(f"ğŸ’¾ Created: {output_path}")
        return output_path

    def validate_dataset(self, filepath: Path) -> Dict:
        """Validate acquired dataset."""
        self.log(f"ğŸ” Validating: {filepath.name}")
        
        zeros = self.parse_odlyzko_format(filepath)
        
        if not zeros:
            return {"valid": False, "error": "No zeros parsed"}
        
        is_sorted = all(zeros[i] <= zeros[i+1] for i in range(len(zeros)-1))
        
        gaps = np.diff(zeros)
        
        validation = {
            "valid": True,
            "count": len(zeros),
            "range": (float(min(zeros)), float(max(zeros))),
            "sorted": is_sorted,
            "gaps": {
                "min": float(np.min(gaps)),
                "max": float(np.max(gaps)),
                "mean": float(np.mean(gaps)),
                "median": float(np.median(gaps)),
            },
            "warnings": []
        }
        
        if not is_sorted:
            validation["warnings"].append("Dataset not sorted")
        
        if validation["gaps"]["min"] < 0.1:
            validation["warnings"].append("Suspiciously small gaps detected")
        
        self.log(f"âœ… Validation complete:")
        self.log(f"   Count: {validation['count']}")
        self.log(f"   Range: {validation['range'][0]:.2f} to {validation['range'][1]:.2f}")
        self.log(f"   Mean gap: {validation['gaps']['mean']:.3f}")
        
        return validation

    def export_metadata(self, acquired_files: List[Path], validations: List[Dict]) -> None:
        """Export metadata about acquired datasets."""
        metadata = {
            "acquisition_date": "2025-11-28",
            "mode": "synthetic" if self.SYNTHETIC_MODE else "download",
            "datasets": []
        }
        
        for filepath, validation in zip(acquired_files, validations):
            metadata["datasets"].append({
                "filename": filepath.name,
                "path": str(filepath),
                "validation": validation
            })
        
        metadata_path = self.output_dir / "acquisition_metadata.json"
        with metadata_path.open('w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
        
        self.log(f"ğŸ“‹ Metadata exported: {metadata_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Acquire large-scale RH zero datasets"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("external"),
        help="Output directory for datasets"
    )
    parser.add_argument(
        "--tables",
        nargs="+",
        default=["zeros1"],
        help="Odlyzko table IDs to acquire"
    )
    parser.add_argument(
        "--include-gourdon",
        action="store_true",
        help="Include Gourdon extended dataset"
    )
    parser.add_argument(
        "--real-download",
        action="store_true",
        help="Attempt real downloads (default: synthetic mode)"
    )
    
    args = parser.parse_args()
    
    print("ğŸ¯ TNFR RH Zeros External Dataset Acquisition")
    print("=" * 55)
    
    # Initialize acquisition
    acquisition = RHZerosAcquisition(output_dir=args.output_dir)
    
    if args.real_download:
        acquisition.SYNTHETIC_MODE = False
        print("ğŸŒ REAL DOWNLOAD MODE")
    else:
        print("ğŸ”¬ SYNTHETIC MODE (for development)")
    
    print(f"ğŸ“ Output directory: {args.output_dir}")
    
    # Acquire Odlyzko tables
    print("\nğŸ“¥ Acquiring Odlyzko tables...")
    acquired_files = acquisition.acquire_odlyzko_tables(args.tables)
    
    # Acquire Gourdon data if requested
    if args.include_gourdon:
        print("\nğŸ“¥ Acquiring Gourdon extended data...")
        gourdon_file = acquisition.create_gourdon_synthetic()
        acquired_files.append(gourdon_file)
    
    # Validate all acquired datasets
    print("\nğŸ” Validating acquired datasets...")
    validations = []
    for filepath in acquired_files:
        validation = acquisition.validate_dataset(filepath)
        validations.append(validation)
        
        if not validation["valid"]:
            print(f"âŒ Validation failed for {filepath.name}: {validation.get('error')}")
    
    # Export metadata
    acquisition.export_metadata(acquired_files, validations)
    
    # Summary
    print("\n" + "=" * 60)
    print("âœ¨ ACQUISITION COMPLETE")
    print("=" * 60)
    
    total_zeros = sum(v["count"] for v in validations if v.get("valid"))
    print(f"ğŸ“Š Total zeros acquired: {total_zeros:,}")
    print(f"ğŸ“ Files created: {len(acquired_files)}")
    print(f"ğŸ’¾ Location: {args.output_dir.absolute()}")
    
    print("\nğŸ”„ Next steps:")
    print("   1. Review datasets in external/")
    print("   2. Run: python zero_database_builder.py --include-builtin \\")
    print(f"           --external {args.output_dir}/odlyzko_*.txt \\")
    if args.include_gourdon:
        print(f"           --external {args.output_dir}/gourdon_*.txt")
    print("   3. Verification scripts will auto-upgrade to larger dataset")
    
    if acquisition.SYNTHETIC_MODE:
        print("\nâš ï¸  NOTE: Synthetic data used for development")
        print("   For production, acquire real Odlyzko/Gourdon tables")


if __name__ == "__main__":
    main()
